{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Q                           A  label\n",
      "0                실수한거 같아                   잘 생각해보세요.      0\n",
      "1            커피 좀 줄여야겠어.                  과해도 안 좋아요.      0\n",
      "2                모른척 해줬어        그게 서로에게 좋았던 선택일 거예요.      0\n",
      "3                   답답해서                 좋은 생각만 하세요.      1\n",
      "4             돈이 천원밖에 없어           돈 없어도 할 수 있는게 많아요      0\n",
      "5     짝사랑만큼 고통스러운 건 없겠지.    짝사랑 만큼 감정소모가 큰 건 없을 거예요.      2\n",
      "6  3달이지났는데 이제야 헤어짐을 확신했네              이제야 실감이 나나 봐요.      1\n",
      "7                    답답해  가까운 곳으로 여행을 가보는 것도 좋을 거예요.      1\n",
      "8                힘드네 장거리        물리적인 거리를 무시하지 못하니까요.      1\n",
      "9              택배 왜 안오지?              송장 번호를 확인해보세요.      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "chatbot_data = pd.read_csv('./korean_chatbot_data/ChatbotData.csv')\n",
    "chatbot_data = chatbot_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(chatbot_data[0: 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "MAX_LEN = 30\n",
    "TIME_STEPS = MAX_LEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def clean_sentence (sentence):\n",
    "  sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]', r'', sentence)\n",
    "  return sentence\n",
    "\n",
    "def process_morph (sentence):\n",
    "  return ' '.join(okt.morphs(sentence))\n",
    "\n",
    "def clean_and_morph(sentence):\n",
    "  sentence = clean_sentence(sentence)\n",
    "  sentence = process_morph(sentence)\n",
    "  return sentence\n",
    "\n",
    "def attach_answer_token (sentence):\n",
    "  return ('<START> ' + sentence, sentence + ' <END>')\n",
    "\n",
    "def preprocess (questions, answers):\n",
    "  ret_questions = []\n",
    "  ret_answer_ins = []\n",
    "  ret_answer_outs = []\n",
    "\n",
    "  for question in questions:\n",
    "    question_processed = process_morph(clean_sentence(question))\n",
    "    ret_questions.append(question_processed)\n",
    "\n",
    "  for answer in answers:\n",
    "    answer_in, answer_out = attach_answer_token(process_morph(clean_sentence(answer)))\n",
    "    ret_answer_ins.append(answer_in)\n",
    "    ret_answer_outs.append(answer_out)\n",
    "  \n",
    "  return ret_questions, ret_answer_ins, ret_answer_outs\n",
    "\n",
    "class TransformUtils:\n",
    "  def __init__ (self, tokenizer, max_len, start_token, end_token):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    self.start_token = start_token\n",
    "    self.end_token = end_token\n",
    "  \n",
    "  def convert_sentences_to_vectors (self, sentences):\n",
    "    ret = []\n",
    "    for i in range(len(sentences)):\n",
    "      sentence = clean_sentence(sentences[i])\n",
    "      sentence = process_morph(sentences[i])\n",
    "      ret.append(sentence)\n",
    "    ret = self.tokenizer.texts_to_sequences(sentences)\n",
    "    ret = pad_sequences(ret, maxlen=self.max_len, truncating='post', padding='post')\n",
    "    ret = tf.convert_to_tensor(ret, dtype=tf.int32)\n",
    "    return ret\n",
    "\n",
    "  def convert_vectors_to_sentences (self, vectors):\n",
    "    ret = []\n",
    "    for vector in vectors:\n",
    "      sentence = ''\n",
    "      for vi in vector:\n",
    "        if vi <= 0 or self.tokenizer.index_word[vi] is None:\n",
    "          sentence += '<None>'\n",
    "        else:\n",
    "          sentence += self.tokenizer.index_word[vi]\n",
    "        sentence += ' '\n",
    "      ret.append(sentence)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['실수 한 거 같아', '커피 좀 줄여야겠어', '모른 척 해줬어', '답답해서', '돈 이 천원 밖에 없어', '짝사랑 만큼 고통스러운 건 없겠지', '3 달이 지났는데 이제야 헤어짐을 확신 했네', '답답해', '힘드네 장거리', '택배 왜 안 오지'] ['<START> 잘 생각 해보세요', '<START> 과 해도 안 좋아요', '<START> 그게 서로 에게 좋았던 선택 일 거 예요', '<START> 좋은 생각 만 하세요', '<START> 돈 없어도 할 수 있는게 많아요', '<START> 짝사랑 만큼 감정 소모 가 큰 건 없을 거 예요', '<START> 이제야 실감 이 나나 봐요', '<START> 가까운 곳 으로 여행 을 가보는 것 도 좋을 거 예요', '<START> 물리 적 인 거리 를 무시 하지 못 하니까 요', '<START> 송장 번호 를 확인 해보세요'] ['잘 생각 해보세요 <END>', '과 해도 안 좋아요 <END>', '그게 서로 에게 좋았던 선택 일 거 예요 <END>', '좋은 생각 만 하세요 <END>', '돈 없어도 할 수 있는게 많아요 <END>', '짝사랑 만큼 감정 소모 가 큰 건 없을 거 예요 <END>', '이제야 실감 이 나나 봐요 <END>', '가까운 곳 으로 여행 을 가보는 것 도 좋을 거 예요 <END>', '물리 적 인 거리 를 무시 하지 못 하니까 요 <END>', '송장 번호 를 확인 해보세요 <END>']\n"
     ]
    }
   ],
   "source": [
    "questions, answer_ins, answer_outs = preprocess(chatbot_data['Q'], chatbot_data['A'])\n",
    "print(questions[0: 10], answer_ins[0: 10], answer_outs[0: 10])\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "all_sentences = questions + answer_ins + answer_outs\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "START_TOKEN = tokenizer.word_index['<START>']\n",
    "END_TOKEN = tokenizer.word_index['<END>']\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_utils = TransformUtils(tokenizer, MAX_LEN, START_TOKEN, END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (Model):\n",
    "  def __init__ (self, input_vocab_size, embedding_dim, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call (self, inputs, state=None):\n",
    "    vectors = self.embedding(inputs)\n",
    "    output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (Model):\n",
    "  def __init__ (self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size, self.embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    self.attention = Attention()\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size, activation='softmax')\n",
    "\n",
    "  def call (self, inputs, enc_outputs, state=None):\n",
    "    vectors = self.embedding(inputs)\n",
    "    rnn_output, rnn_output_state = self.gru(vectors, initial_state=state)\n",
    "    context_vector = self.attention(inputs=[rnn_output, enc_outputs])\n",
    "    context_and_output = tf.concat([rnn_output, context_vector], axis=-1)\n",
    "    attention_vector = self.fc(context_and_output)\n",
    "    return attention_vector, rnn_output_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTrainer (Model):\n",
    "  def __init__ (self, vocab_size, embedding_dim, units):\n",
    "    super(Seq2SeqTrainer, self).__init__()\n",
    "    self.encoder = Encoder(vocab_size, embedding_dim, units)\n",
    "    self.decoder = Decoder(vocab_size, embedding_dim, units)\n",
    "\n",
    "  def __init__ (self, encoder, decoder):\n",
    "    super(Seq2SeqTrainer, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def call (self, inputs):\n",
    "    questions, answer_ins = inputs\n",
    "    enc_outputs, enc_state = self.encoder(questions)\n",
    "    dec_result, dec_state = self.decoder(answer_ins, enc_outputs, state=enc_state)\n",
    "    return dec_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoss (tf.keras.losses.Loss):\n",
    "  def __init__ (self):\n",
    "    super(TrainLoss, self).__init__()\n",
    "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  \n",
    "  def call (self, y_t, y_pred):\n",
    "    loss = self.loss(y_t, y_pred)\n",
    "    return tf.reduce_sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "  def __init__ (self, encoder, decoder, transform_utils, start_token, end_token):\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.transform_utils = transform_utils\n",
    "    self.start_token = start_token\n",
    "    self.end_token = end_token\n",
    "  \n",
    "  def translate (self, inputs):\n",
    "    return transform_utils.convert_vectors_to_sentences(self.get_translated_vectors(inputs))\n",
    "  \n",
    "  def get_translated_vectors (self, inputs):\n",
    "    ret = []\n",
    "    vectors = transform_utils.convert_sentences_to_vectors(inputs)\n",
    "    for vector in vectors:\n",
    "      ret_vector = [self.start_token]\n",
    "      enc_output, enc_state = self.encoder(tf.convert_to_tensor([vector], dtype=tf.int32))\n",
    "      dec_state = enc_state\n",
    "      cnt = 0\n",
    "      while ret_vector[-1] != self.end_token and cnt < 30:\n",
    "        dec_result, dec_state = self.decoder(tf.convert_to_tensor([[ret_vector[-1]]], dtype=tf.int32), enc_output, state=dec_state)\n",
    "        ret_vector.append(tf.math.argmax(dec_result[0][0]).numpy())\n",
    "        cnt += 1\n",
    "      ret.append(ret_vector)\n",
    "      \n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    multiple                  3235328   \n",
      "                                                                 \n",
      " gru_10 (GRU)                multiple                  3938304   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,173,632\n",
      "Trainable params: 7,173,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    multiple                  3235328   \n",
      "                                                                 \n",
      " gru_11 (GRU)                multiple                  3938304   \n",
      "                                                                 \n",
      " attention_5 (Attention)     multiple                  0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  25895262  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,068,894\n",
      "Trainable params: 33,068,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"seq2_seq_trainer_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_5 (Encoder)         ((None, 30, 1024),        7173632   \n",
      "                              (None, 1024))                      \n",
      "                                                                 \n",
      " decoder_5 (Decoder)         ((None, 30, 12638),       33068894  \n",
      "                              (None, 1024))                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,242,526\n",
      "Trainable params: 40,242,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "185/185 [==============================] - 10s 48ms/step - loss: 1.6235 - acc: 0.7965\n",
      "Epoch 2/20\n",
      "185/185 [==============================] - 9s 48ms/step - loss: 1.1897 - acc: 0.8270\n",
      "Epoch 3/20\n",
      "185/185 [==============================] - 9s 48ms/step - loss: 1.0616 - acc: 0.8386\n",
      "Epoch 4/20\n",
      "185/185 [==============================] - 9s 48ms/step - loss: 0.9507 - acc: 0.8487\n",
      "Epoch 5/20\n",
      "185/185 [==============================] - 9s 48ms/step - loss: 0.8463 - acc: 0.8578\n",
      "Epoch 6/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.7439 - acc: 0.8690\n",
      "Epoch 7/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.6450 - acc: 0.8808\n",
      "Epoch 8/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.5547 - acc: 0.8935\n",
      "Epoch 9/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.4743 - acc: 0.9060\n",
      "Epoch 10/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.4033 - acc: 0.9179\n",
      "Epoch 11/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.3410 - acc: 0.9282\n",
      "Epoch 12/20\n",
      "185/185 [==============================] - 9s 48ms/step - loss: 0.2830 - acc: 0.9383\n",
      "Epoch 13/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.2298 - acc: 0.9483\n",
      "Epoch 14/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.1806 - acc: 0.9583\n",
      "Epoch 15/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.1379 - acc: 0.9680\n",
      "Epoch 16/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.1019 - acc: 0.9763\n",
      "Epoch 17/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.0752 - acc: 0.9824\n",
      "Epoch 18/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.0601 - acc: 0.9863\n",
      "Epoch 19/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.0473 - acc: 0.9893\n",
      "Epoch 20/20\n",
      "185/185 [==============================] - 9s 49ms/step - loss: 0.0348 - acc: 0.9923\n",
      "{'loss': [1.6235204935073853, 1.1897157430648804, 1.0616074800491333, 0.9507240056991577, 0.8463077545166016, 0.7439461946487427, 0.6449618935585022, 0.554665207862854, 0.47431039810180664, 0.4033089578151703, 0.34097129106521606, 0.28300511837005615, 0.22977197170257568, 0.18059340119361877, 0.13789810240268707, 0.10194158554077148, 0.07515791803598404, 0.060093361884355545, 0.04731462895870209, 0.03484278544783592], 'acc': [0.7965237498283386, 0.8269587755203247, 0.8386111855506897, 0.848735511302948, 0.8577800393104553, 0.8689785599708557, 0.8807578682899475, 0.8934703469276428, 0.9060305953025818, 0.9179339408874512, 0.9282077550888062, 0.9383208155632019, 0.9482928514480591, 0.9582762122154236, 0.968019962310791, 0.9762891530990601, 0.9824466705322266, 0.9862979054450989, 0.989314615726471, 0.9923425912857056]}\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "question_vectors, answer_in_vectors, answer_out_vectors = \\\n",
    "  transform_utils.convert_sentences_to_vectors(questions), \\\n",
    "  transform_utils.convert_sentences_to_vectors(answer_ins), \\\n",
    "  transform_utils.convert_sentences_to_vectors(answer_outs)\n",
    "\n",
    "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "encoder(Input(shape=(TIME_STEPS, ), dtype=tf.int32))\n",
    "encoder.summary()\n",
    "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "decoder(Input(shape=(TIME_STEPS, ), dtype=tf.int32), Input(shape=(TIME_STEPS, UNITS, ), dtype=tf.float32), state=Input(shape=(UNITS, ), dtype=tf.float32))\n",
    "decoder.summary()\n",
    "trainer = Seq2SeqTrainer(encoder, decoder)\n",
    "trainer([Input(shape=(TIME_STEPS, ), dtype=tf.int32), Input(shape=(TIME_STEPS, ), dtype=tf.int32)])\n",
    "trainer.summary()\n",
    "trainer.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=TrainLoss(), metrics=['acc'])\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "  history = trainer.fit([question_vectors, answer_in_vectors], answer_out_vectors, batch_size=BATCH_SIZE, epochs=20, validation_split=0)\n",
    "print(history.history)\n",
    "\n",
    "trainer.save_weights('./model/trainer')\n",
    "\n",
    "encoder.save_weights('./model/translator/encoder')\n",
    "decoder.save_weights('./model/translator/decoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    multiple                  3235328   \n",
      "                                                                 \n",
      " gru_12 (GRU)                multiple                  3938304   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,173,632\n",
      "Trainable params: 7,173,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_13 (Embedding)    multiple                  3235328   \n",
      "                                                                 \n",
      " gru_13 (GRU)                multiple                  3938304   \n",
      "                                                                 \n",
      " attention_6 (Attention)     multiple                  0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  25895262  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,068,894\n",
      "Trainable params: 33,068,894\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "encoder.load_weights('./model/translator/encoder')\n",
    "encoder(Input(shape=(TIME_STEPS, ), dtype=tf.int32))\n",
    "encoder.summary()\n",
    "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "decoder.load_weights('./model/translator/decoder')\n",
    "decoder(Input(shape=(TIME_STEPS, ), dtype=tf.int32), Input(shape=(TIME_STEPS, UNITS, ), dtype=tf.float32), state=Input(shape=(UNITS, ), dtype=tf.float32))\n",
    "decoder.summary()\n",
    "\n",
    "translator = Translator(encoder, decoder, transform_utils, START_TOKEN, END_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뭐 할까: <START> 저 랑 놀아요 <END> \n",
      "커피 마시고 싶다: <START> 저 도 커피 좋아해요 <END> \n",
      "인 성 문제 있는거 아니야: <START> 새로운 무언가 를 해보는건 좋겠어요 <END> \n",
      "공부 하기 싫다: <START> 같이 수다 떨면서 놀까 요 <END> \n",
      "숙제 가 너무 많다: <START> 미리 미리 해야죠 <END> \n",
      "먹을 꺼 추천 좀: <START> 냉장고 파먹기 해보세요 <END> \n",
      "재미 난 거 없나: <START> 한번 더 울어 보세요 <END> \n",
      "주식 공부 할 까: <START> 남 에게 피 할 수 있을 거 예요 <END> \n",
      "가스 불 켜고 나갔어: <START> 빨리 집 에 돌아가서 끄고 나오세요 <END> \n",
      "우리 집 에 불 났어: <START> 집 에 도움 이 되려고 노력 해보세요 <END> \n",
      "여행 가고 싶습니다: <START> 좋은 여행 되길 바랍니다 <END> \n",
      "축구 하고 싶다: <START> 잘 될 거 예요 <END> \n",
      "술 중독 인거 같아: <START> 술 많이 드시면 더 무너져요 <END> \n",
      "나 이상한가: <START> 지극히 평범하면서 지극히 특별하죠 <END> \n",
      "나 잘 살 수 있겠지: <START> 잘 살 수 있을 거 예요 <END> \n",
      "집에 가고 싶다: <START> 미리 충전 하세요 <END> \n",
      "배 타고 싶다: <START> 어찌 하면 좋을까요 <END> \n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "  '뭐 할까',\n",
    "  '커피 마시고 싶다',\n",
    "  '인 성 문제 있는거 아니야',\n",
    "  '공부 하기 싫다',\n",
    "  '숙제 가 너무 많다',\n",
    "  '먹을 꺼 추천 좀',\n",
    "  '재미 난 거 없나',\n",
    "  '주식 공부 할 까',\n",
    "  '가스 불 켜고 나갔어',\n",
    "  '우리 집 에 불 났어',\n",
    "  '여행 가고 싶습니다',\n",
    "  '축구 하고 싶다',\n",
    "  '술 중독 인거 같아',\n",
    "  '나 이상한가',\n",
    "  '나 잘 살 수 있겠지',\n",
    "  '집에 가고 싶다',\n",
    "  '배 타고 싶다'\n",
    "]\n",
    "# print('\\n'.join(transform_utils.convert_vectors_to_sentences(\n",
    "#   transform_utils.convert_sentences_to_vectors(sentences).numpy()\n",
    "# )))\n",
    "# print('\\n')\n",
    "# print('\\n'.join(translator.translate(sentences)))\n",
    "for sentence in sentences:\n",
    "  print(sentence + ':', end=' ')\n",
    "  print(''.join(translator.translate([sentence])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec62f2ec7c7d49224ec335e3ef472b8ef537dc7949933ed74784e8710c2a92fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
