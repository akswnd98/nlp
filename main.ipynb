{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "chatbot_data = pd.read_csv('./korean_chatbot_data/ChatbotData.csv')\n",
    "chatbot_data = chatbot_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(chatbot_data[0: 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "MAX_LEN = 30\n",
    "TIME_STEPS = MAX_LEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def clean_sentence (sentence):\n",
    "  sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]', r'', sentence)\n",
    "  return sentence\n",
    "\n",
    "def process_morph (sentence):\n",
    "  return ' '.join(okt.morphs(sentence))\n",
    "\n",
    "def clean_and_morph(sentence):\n",
    "  sentence = clean_sentence(sentence)\n",
    "  sentence = process_morph(sentence)\n",
    "  return sentence\n",
    "\n",
    "def attach_answer_token (sentence):\n",
    "  return ('<START> ' + sentence, sentence + ' <END>')\n",
    "\n",
    "def preprocess (questions, answers):\n",
    "  ret_questions = []\n",
    "  ret_answer_ins = []\n",
    "  ret_answer_outs = []\n",
    "\n",
    "  for question in questions:\n",
    "    question_processed = process_morph(clean_sentence(question))\n",
    "    ret_questions.append(question_processed)\n",
    "\n",
    "  for answer in answers:\n",
    "    answer_in, answer_out = attach_answer_token(process_morph(clean_sentence(answer)))\n",
    "    ret_answer_ins.append(answer_in)\n",
    "    ret_answer_outs.append(answer_out)\n",
    "  \n",
    "  return ret_questions, ret_answer_ins, ret_answer_outs\n",
    "\n",
    "class TransformUtils:\n",
    "  def __init__ (self, tokenizer, max_len, start_token, end_token):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    self.start_token = start_token\n",
    "    self.end_token = end_token\n",
    "  \n",
    "  def convert_sentences_to_vectors (self, sentences):\n",
    "    ret = []\n",
    "    for i in range(len(sentences)):\n",
    "      sentence = clean_sentence(sentences[i])\n",
    "      sentence = process_morph(sentences[i])\n",
    "      ret.append(sentence)\n",
    "    ret = self.tokenizer.texts_to_sequences(sentences)\n",
    "    ret = pad_sequences(ret, maxlen=self.max_len, truncating='post', padding='post')\n",
    "    ret = tf.convert_to_tensor(ret)\n",
    "    return ret\n",
    "\n",
    "  def convert_vectors_to_sentences (self, vectors):\n",
    "    ret = []\n",
    "    for vector in vectors:\n",
    "      sentence = ''\n",
    "      for vi in vector:\n",
    "        if vi <= 0 or self.tokenizer.index_word[vi.numpy()] is None:\n",
    "          sentence += '<None>'\n",
    "        else:\n",
    "          sentence += self.tokenizer.index_word[vi.numpy()]\n",
    "        sentence += ' '\n",
    "      ret.append(sentence)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answer_ins, answer_outs = preprocess(chatbot_data['Q'], chatbot_data['A'])\n",
    "print(questions[0: 10], answer_ins[0: 10], answer_outs[0: 10])\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "all_sentences = questions + answer_ins + answer_outs\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "START_TOKEN = tokenizer.word_index['<START>']\n",
    "END_TOKEN = tokenizer.word_index['<END>']\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_utils = TransformUtils(tokenizer, MAX_LEN, START_TOKEN, END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "  def __init__ (self, input_vocab_size, embedding_dim, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call (self, inputs, state=None):\n",
    "    vectors = self.embedding(inputs)\n",
    "    output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (Model):\n",
    "  def __init__ (self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size, self.embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    self.attention = Attention()\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size, activation='softmax')\n",
    "  \n",
    "  def call (self, inputs, enc_outputs, state=None):\n",
    "    vectors = self.embedding(inputs)\n",
    "    rnn_output, rnn_output_state = self.gru(vectors, initial_state=state)\n",
    "    # <마지막 rnn_output 대신 initial_state를 넣어 준 버전>\n",
    "    # attention_inputs = tf.concat([state[:, tf.newaxis, :], rnn_output[:, : -1, :]], axis=1)\n",
    "    # context_vector = self.attention(inputs=[attention_inputs, enc_outputs])\n",
    "    # <마지막 rnn_output 대신 initial_state를 넣어 준 버전 />\n",
    "    context_vector = self.attention(inputs=[rnn_output, enc_outputs])\n",
    "    context_and_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "    attention_vector = self.fc(context_and_output)\n",
    "    return attention_vector, rnn_output_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTrainer (Model):\n",
    "  def __init__ (self, encoder, decoder):\n",
    "    super(Seq2SeqTrainer, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def call (self, inputs):\n",
    "    questions, answer_ins = inputs\n",
    "    enc_outputs, enc_state = self.encoder(questions)\n",
    "    dec_result, dec_state = self.decoder(answer_ins, enc_outputs, state=enc_state)\n",
    "    return dec_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoss (tf.keras.losses.Loss):\n",
    "  def __init__ (self):\n",
    "    super(TrainLoss, self).__init__()\n",
    "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  \n",
    "  def call (self, y_t, y_pred):\n",
    "    loss = self.loss(y_t, y_pred)\n",
    "    return tf.reduce_sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vectors, answer_in_vectors, answer_out_vectors = \\\n",
    "  transform_utils.convert_sentences_to_vectors(questions), \\\n",
    "  transform_utils.convert_sentences_to_vectors(answer_ins), \\\n",
    "  transform_utils.convert_sentences_to_vectors(answer_outs)\n",
    "\n",
    "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "encoder(Input(shape=(TIME_STEPS, )))\n",
    "encoder.summary()\n",
    "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "decoder(Input(shape=(TIME_STEPS, )), Input(shape=(TIME_STEPS, UNITS, )), state=Input(shape=(UNITS, )))\n",
    "decoder.summary()\n",
    "trainer = Seq2SeqTrainer(encoder, decoder)\n",
    "trainer([Input(shape=(TIME_STEPS, )), Input(shape=(TIME_STEPS, ))])\n",
    "trainer.summary()\n",
    "trainer.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss=TrainLoss())\n",
    "\n",
    "for idx in range(0, len(question_vectors) - BATCH_SIZE, BATCH_SIZE):\n",
    "  if idx % (BATCH_SIZE * 100) == 0:\n",
    "    tf.saved_model.save(encoder, '.\\model\\encoder')\n",
    "    tf.saved_model.save(decoder, '.\\model\\decoder')\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    history = trainer.fit([question_vectors[idx: idx + BATCH_SIZE], answer_in_vectors[idx: idx + BATCH_SIZE]], answer_out_vectors[idx: idx + BATCH_SIZE])\n",
    "  print(idx, history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "# tf.saved_model.load('.\\model\\encoder')\n",
    "\n",
    "# decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "# tf.saved_model.load('.\\model\\decoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "  def __init__ (self, encoder, decoder, transform_utils, start_token, end_token):\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.transform_utils = transform_utils\n",
    "    self.start_token = start_token\n",
    "    self.end_token = end_token\n",
    "  \n",
    "  def translate (self, inputs):\n",
    "    return self.transform_utils.convert_vectors_to_sentences(self.get_translated_vector(inputs))\n",
    "    \n",
    "  def get_translated_vector (self, inputs):\n",
    "    vectors = self.transform_utils.convert_sentences_to_vectors(inputs)\n",
    "    ret = []\n",
    "    for vector in vectors:\n",
    "      ret_vector = [self.start_token]\n",
    "      enc_output, enc_state = self.encoder(tf.convert_to_tensor([vector]))\n",
    "      dec_state = enc_state\n",
    "      cnt = 0\n",
    "      while ret_vector[-1] != self.end_token and cnt < 30:\n",
    "        dec_result, dec_state = self.decoder(tf.convert_to_tensor([[ret_vector[-1]]]), enc_output, state=dec_state)\n",
    "        ret_vector.append(tf.math.argmax(dec_result[0][0]).numpy())\n",
    "        cnt += 1\n",
    "      ret.append(ret_vector)\n",
    "      \n",
    "    return tf.convert_to_tensor(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "# decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
    "translator = Translator(encoder, decoder, transform_utils, START_TOKEN, END_TOKEN)\n",
    "\n",
    "# sentences = ['오늘 강 추위 래 요']\n",
    "sentences = ['커피 마시고 싶다']\n",
    "# sentences = ['고민 상담 좀']\n",
    "# sentences = ['인성 문제 있는거 아니야']\n",
    "# sentences = ['공부 하기 싫다']\n",
    "print(\n",
    "  transform_utils.convert_vectors_to_sentences(\n",
    "    transform_utils.convert_sentences_to_vectors(sentences)\n",
    "  )\n",
    ")\n",
    "print(translator.translate(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec62f2ec7c7d49224ec335e3ef472b8ef537dc7949933ed74784e8710c2a92fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
